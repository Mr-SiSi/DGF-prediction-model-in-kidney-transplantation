{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "620bd177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.cm import rainbow\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import shap\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c020c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Your path files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b9cf46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29bd54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a41d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#卡方检验（比例模型）\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "data_a1 = dataset[dataset['Outcome'] == 0].iloc[:, 2:10]#####取outcome是0的二分类变量\n",
    "data_b1 = dataset[dataset['Outcome'] == 1].iloc[:, 2:10]#####取outcome是1的二分类变量\n",
    "\n",
    "#a = np.arange(9).reshape(3,3)\n",
    "#定义数组\n",
    "a = np.arange(data_a1.shape[1])\n",
    "b = np.arange(data_a1.shape[1])\n",
    "for i in range(0,data_a1.shape[1]):\n",
    "    a[i] = data_a1.iloc[:,i].sum()\n",
    "    b[i] = data_b1.iloc[:,i].sum()\n",
    "\n",
    "chi_data = np.arange(6).reshape(3,2)\n",
    "\n",
    "for i in range(0,data_a1.shape[1]):\n",
    "    chi_data = np.array([[a[i],b[i],a[i]+b[i]],[data_a1.shape[0],data_b1.shape[0],data_a1.shape[0]+data_b1.shape[0]]])\n",
    "    data = dataset.iloc[:, i+2]\n",
    "    print(data.name)\n",
    "    print(chi_data)\n",
    "    stat, p, dof, expected = chi2_contingency(chi_data)\n",
    "    \n",
    "    #第一种print\n",
    "    #print('自由度dof=%d' % dof)\n",
    "    #print('期望分布')\n",
    "    #print(expected)\n",
    "    \n",
    "    prob = 0.95\n",
    "    critical = chi2.ppf(prob, dof)\n",
    "    #第二种print\n",
    "    #print('probability=%.3f, critical=%.3f, stat=%.3f' % (prob, critical, stat))\n",
    "    #if abs(stat) >= critical:\n",
    "    #    print('拒绝原假设，两变量存在相关关系')\n",
    "    #else:\n",
    "    #    print('不能拒绝原假设，两变量相互独立')\n",
    "    \n",
    "    #第三种带有p值的print\n",
    "    alpha = 1.0 - prob\n",
    "    print('significance=%.3f, p=%.3f' % (alpha, p))\n",
    "    if p <= alpha:\n",
    "        print('拒绝原假设，两变量存在相关关系')\n",
    "    else:\n",
    "        print('不能拒绝原假设，两变量相互独立')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819fd1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#正态分布检验：W检验 Shapiro-Wilk检验\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "for i in range(1,15):\n",
    "    data = dataset.iloc[:, i+9]\n",
    "    stat,p = shapiro(data)\n",
    "    print(data.name)\n",
    "    print(\"stat为：%f\" %stat,\"p值为：%f\" %p)\n",
    "    if p > 0.05:\n",
    "        print('不能拒绝原假设，样本数据服从正态分布')\n",
    "    else:\n",
    "        print('不服从正态分布')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03045f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t检验，本章使用的是两独立样本t检验\n",
    "from scipy.stats import ttest_ind\n",
    "#from scipy.stats import ttest_rel 配合下面的检验类型使用对应函数包\n",
    "#from scipy.stats import ttest_1samp\n",
    "#from scipy.stats import f_oneway 多样本方差分析\n",
    "\n",
    "data_a = dataset[dataset['Outcome'] == 0]\n",
    "data_b = dataset[dataset['Outcome'] == 1]\n",
    "\n",
    "for i in range(1,14):\n",
    "    t = i+9\n",
    "    data1 = data_a.iloc[:, t]\n",
    "    data2 = data_b.iloc[:, t]\n",
    "    stat, p = ttest_ind(data1, data2)\n",
    "    #如果是配对关系样本 使用stat, p = ttest_rel(data1, data2)\n",
    "    #单样本 t检验，只有一个data应该使用stats.ttest_1samp(data, 1)\n",
    "    #多样本方差分析 stat, p = f_oneway(data1, data2, data3)\n",
    "    print(data1.name,'的结果是')\n",
    "    print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "    if p > 0.05:\n",
    "        print('不能拒绝原假设，两样本集分布相同')\n",
    "    else:\n",
    "        print('拒绝原假设，样本集分布可能不同')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d733ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#非参数检验，适用于非正态分布的数据，本文使用U检验\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "data_a = dataset[dataset['Outcome'] == 0]\n",
    "data_b = dataset[dataset['Outcome'] == 1]\n",
    "\n",
    "for i in range(1,14):\n",
    "    t = i+9\n",
    "    data1 = data_a.iloc[:, t]\n",
    "    data2 = data_b.iloc[:, t]\n",
    "    stat, p = mannwhitneyu(data1, data2)\n",
    "    \n",
    "    print(data1.name,'的结果是')\n",
    "    print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "    if p > 0.05:\n",
    "        print('不能拒绝原假设，两样本集分布相同')\n",
    "    else:\n",
    "        print('拒绝原假设，样本集分布可能不同')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff3d544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f767e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log归一化处理 结果差不多，一般不用，使用标准化特征预处理（naive bayes）结果能够有效提升\n",
    "dataset.iloc[:,1:58]= dataset.iloc[:,1:58].apply(np.log10)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4fb9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果没有使用log归一化，可以不用\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52931647",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 40, 80\n",
    "plt.matshow(dataset.corr('kendall'), cmap=plt.cm.seismic)\n",
    "#默认为pearson相关系数，完整代码为dataset.corr(method='pearson', min_periods=1),还有kendall：用于反映分类变量相关性的指标，即针对无序序列的相关系数，非正太分布的数据\n",
    "#以及spearman：非线性的，非正太分析的数据的相关系数 后面的cmap是颜色，自己搜索颜色对应命令可以调节\n",
    "plt.yticks(np.arange(dataset.shape[1]), dataset.columns)\n",
    "plt.xticks(np.arange(dataset.shape[1]), dataset.columns, rotation=90)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6992f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.hist()\n",
    "a = dataset.corr()\n",
    "a.to_csv('/Users/henryding/Library/CloudStorage/OneDrive-cmu.edu.cn/sci/复杂网络动态模型/supplementary/Clean Dataset coefficient(pearson).csv')\n",
    "\n",
    "b = dataset.corr('kendall')\n",
    "b.to_csv('/Users/henryding/Library/CloudStorage/OneDrive-cmu.edu.cn/sci/复杂网络动态模型/supplementary/Clean Dataset coefficient(kendall).csv')\n",
    "\n",
    "c = dataset.corr('spearman')\n",
    "c.to_csv('/Users/henryding/Library/CloudStorage/OneDrive-cmu.edu.cn/sci/复杂网络动态模型/supplementary/Clean Dataset coefficient(spearman).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ba959",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[\"Outcome\"].describe()\n",
    "dataset[\"Outcome\"].unique()\n",
    "print(dataset.corr()[\"Outcome\"].abs().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163427a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Outcome\"].describe()\n",
    "dataset[\"Outcome\"].unique()\n",
    "print(dataset.corr('kendall')[\"Outcome\"].abs().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b115811",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Outcome\"].describe()\n",
    "dataset[\"Outcome\"].unique()\n",
    "print(dataset.corr('spearman')[\"Outcome\"].abs().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4978e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset[\"Outcome\"]\n",
    "\n",
    "sns.countplot(y)\n",
    "\n",
    "Target_temp = dataset.Outcome.value_counts()\n",
    "\n",
    "print(Target_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "052b7dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "predictors = dataset.drop(\"Outcome\",axis=1)\n",
    "Target = dataset[\"Outcome\"]\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(predictors,Target,test_size=0.40,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a13f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape\n",
    "X_test.shape\n",
    "Y_train.shape\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1fe463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#逻辑回归分类器\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred_lr = lr.predict(X_test)\n",
    "Y_pred_lr.shape\n",
    "score_lr = round(accuracy_score(Y_pred_lr,Y_test)*100,2)\n",
    "\n",
    "print(\"The accuracy score achieved using Logistic Regression is: \"+str(score_lr)+\" %\")\n",
    "# save the model to disk\n",
    "filename = 'LogisticRegression.sav'\n",
    "pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression visualization\n",
    "#为了不干扰后面继续使用dataset，进行一下copy\n",
    "dataset_all = dataset\n",
    "sns.pairplot(data=dataset_all,diag_kind='hist', hue= 'Outcome')\n",
    "plt.show()\n",
    "#plt.savefig('/Users/henryding/Desktop/plot.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_features = dataset_all.iloc[:,0:23]\n",
    "print(dataset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e6123",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dataset_features.columns:\n",
    "    sns.boxplot(x='Outcome', y=col, data=dataset_all)\n",
    "    plt.title(col)\n",
    "    fig = plt.gcf()#注意，下面三行一起用于导出plt结果为pdf\n",
    "    plt.show()\n",
    "    fig.savefig(\"/Users/henryding/Desktop/plot.pdf\", format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08755046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#贝叶斯，高斯模型分类器（概率模型，很快）(高斯朴素贝叶斯分类模型)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "\n",
    "nb.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred_nb = nb.predict(X_test)\n",
    "\n",
    "score_nb = round(accuracy_score(Y_pred_nb,Y_test)*100,2)\n",
    "\n",
    "print(\"The accuracy score achieved using Naive Bayes is: \"+str(score_nb)+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db17eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#标准化特征缩放（standardization feature scaling）\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Code source: Tyler Lanigan <tylerlanigan@gmail.com>\n",
    "#              Sebastian Raschka <mail@sebastianraschka.com>\n",
    "\n",
    "# License: BSD 3 clause\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "FIG_SIZE = (10, 7)\n",
    "\n",
    "\n",
    "# Fit to data and predict using pipelined GNB and PCA\n",
    "unscaled_clf = make_pipeline(PCA(n_components=2), GaussianNB())\n",
    "unscaled_clf.fit(X_train, Y_train)\n",
    "pred_test = unscaled_clf.predict(X_test)\n",
    "\n",
    "# Fit to data and predict using pipelined scaling, GNB and PCA\n",
    "std_clf = make_pipeline(StandardScaler(), PCA(n_components=2), GaussianNB())\n",
    "std_clf.fit(X_train, Y_train)\n",
    "pred_test_std = std_clf.predict(X_test)\n",
    "\n",
    "# Show prediction accuracies in scaled and unscaled data.\n",
    "print(\"\\nPrediction accuracy for the normal test dataset with PCA\")\n",
    "print(f\"{accuracy_score(Y_test, pred_test):.2%}\\n\")\n",
    "\n",
    "print(\"\\nPrediction accuracy for the standardized test dataset with PCA\")\n",
    "print(f\"{accuracy_score(Y_test, pred_test_std):.2%}\\n\")\n",
    "\n",
    "# Extract PCA from pipeline\n",
    "pca = unscaled_clf.named_steps[\"pca\"]\n",
    "pca_std = std_clf.named_steps[\"pca\"]\n",
    "\n",
    "# Show first principal components\n",
    "print(f\"\\nPC 1 without scaling:\\n{pca.components_[0]}\")\n",
    "print(f\"\\nPC 1 with scaling:\\n{pca_std.components_[0]}\")\n",
    "\n",
    "# Use PCA without and with scale on X_train data for visualization.\n",
    "X_train_transformed = pca.transform(X_train)\n",
    "\n",
    "scaler = std_clf.named_steps[\"standardscaler\"]\n",
    "scaled_X_train = scaler.transform(X_train)\n",
    "X_train_std_transformed = pca_std.transform(scaled_X_train)\n",
    "\n",
    "# visualize standardized vs. untouched dataset with PCA performed\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=FIG_SIZE)\n",
    "\n",
    "target_classes = range(0, 2)\n",
    "colors = (\"blue\", \"red\")\n",
    "markers = (\"^\", \"s\")\n",
    "\n",
    "for target_class, color, marker in zip(target_classes, colors, markers):\n",
    "    ax1.scatter(\n",
    "        x=X_train_transformed[Y_train == target_class, 0],\n",
    "        y=X_train_transformed[Y_train == target_class, 1],\n",
    "        color=color,\n",
    "        label=f\"class {target_class}\",\n",
    "        alpha=0.8,\n",
    "        marker=marker,\n",
    "    )\n",
    "\n",
    "    ax2.scatter(\n",
    "        x=X_train_std_transformed[Y_train == target_class, 0],\n",
    "        y=X_train_std_transformed[Y_train == target_class, 1],\n",
    "        color=color,\n",
    "        label=f\"class {target_class}\",\n",
    "        alpha=0.8,\n",
    "        marker=marker,\n",
    "    )\n",
    "\n",
    "ax1.set_title(\"Training dataset after PCA\")\n",
    "ax2.set_title(\"Standardized training dataset after PCA\")\n",
    "\n",
    "for ax in (ax1, ax2):\n",
    "    ax.set_xlabel(\"1st principal component\")\n",
    "    ax.set_ylabel(\"2nd principal component\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b4388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM（支持向量机）分类器——大量运算\n",
    "from sklearn import svm\n",
    "\n",
    "sv = svm.SVC(kernel='linear')\n",
    "\n",
    "sv.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_svm = sv.predict(X_test)\n",
    "\n",
    "Y_pred_svm.shape\n",
    "\n",
    "score_svm = round(accuracy_score(Y_pred_svm,Y_test)*100,2)\n",
    "\n",
    "print(\"The accuracy score achieved using Linear SVM is: \"+str(score_svm)+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6960386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN分类器\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train,Y_train)\n",
    "Y_pred_knn=knn.predict(X_test)\n",
    "\n",
    "Y_pred_knn.shape\n",
    "\n",
    "score_knn = round(accuracy_score(Y_pred_knn,Y_test)*100,2)\n",
    "\n",
    "print(\"The accuracy score achieved using KNN is: \"+str(score_knn)+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707b7b41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#决策树分类器\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "max_accuracy = 0\n",
    "\n",
    "\n",
    "for x in range(200):\n",
    "    dt = DecisionTreeClassifier(random_state=x)\n",
    "    dt.fit(X_train,Y_train)\n",
    "    Y_pred_dt = dt.predict(X_test)\n",
    "    current_accuracy = round(accuracy_score(Y_pred_dt,Y_test)*100,2)\n",
    "    if(current_accuracy>max_accuracy):\n",
    "        max_accuracy = current_accuracy\n",
    "        best_x = x\n",
    "        \n",
    "#print(max_accuracy)\n",
    "#print(best_x)\n",
    "\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=best_x)\n",
    "dt.fit(X_train,Y_train)\n",
    "Y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "print(Y_pred_dt.shape)\n",
    "\n",
    "score_dt = round(accuracy_score(Y_pred_dt,Y_test)*100,2)\n",
    "\n",
    "print(\"The accuracy score achieved using Decision Tree is: \"+str(score_dt)+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机森林分类器\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "max_accuracy = 0\n",
    "\n",
    "\n",
    "for x in range(2000):\n",
    "    rf = RandomForestClassifier(random_state=x)\n",
    "    rf.fit(X_train,Y_train)\n",
    "    Y_pred_rf = rf.predict(X_test)\n",
    "    current_accuracy = round(accuracy_score(Y_pred_rf,Y_test)*100,2)\n",
    "    if(current_accuracy>max_accuracy):\n",
    "        max_accuracy = current_accuracy\n",
    "        best_x = x\n",
    "        \n",
    "#print(max_accuracy)\n",
    "#print(best_x)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=best_x)\n",
    "rf.fit(X_train,Y_train)\n",
    "Y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "Y_pred_rf.shape\n",
    "\n",
    "score_rf = round(accuracy_score(Y_pred_rf,Y_test)*100,2)\n",
    "\n",
    "print(\"The accuracy score achieved using Decision Tree is: \"+str(score_rf)+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066286a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#神经网络（neural network models）\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nnm = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "nnm.fit(X_train,Y_train)\n",
    "Y_pred_nnm = nnm.predict(X_test)\n",
    "\n",
    "Y_pred_nnm.shape\n",
    "\n",
    "score_nnm = round(accuracy_score(Y_pred_nnm,Y_test)*100,2)\n",
    "\n",
    "print(\"The accuracy score achieved using Neural Network is: \"+str(score_nnm)+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3587d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGboost分类器——类似stacking原理，合并了多个弱分类器的强分类器\n",
    "\n",
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "model = xgb_model.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_xgb = xgb_model.predict(X_test)\n",
    "Y_pred_xgb.shape\n",
    "\n",
    "score_xgb = round(accuracy_score(Y_pred_xgb,Y_test)*100,2)\n",
    "\n",
    "print(\"The accuracy score achieved using XGBoost is: \"+str(score_xgb)+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cab0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGboost内容导出，获得权重模型\n",
    "#SHAP处理\n",
    "\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_train)\n",
    "shap.plots.waterfall(shap_values[0])#可视化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70fd80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb41b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aecb2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
